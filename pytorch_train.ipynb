{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"pytorch_train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"papermill":{"default_parameters":{},"duration":2135.890053,"end_time":"2021-10-25T00:51:48.954153","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-10-25T00:16:13.064100","version":"2.3.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"M7FhFJaeX4X-"},"source":["# Sartorius - PyTorch Mask R-CNN training"],"id":"M7FhFJaeX4X-"},{"cell_type":"markdown","metadata":{"id":"Oft72sMpX4YE"},"source":["## Libraries"],"id":"Oft72sMpX4YE"},{"cell_type":"code","metadata":{"id":"jFa0zAg3X4YE"},"source":["DEBUG = False\n","KAGGLE = False\n","COLAB = True"],"id":"jFa0zAg3X4YE","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fzn7vlweYh25"},"source":["from psutil import virtual_memory, cpu_count\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","    print('Not connected to a GPU')\n","else:\n","    print(gpu_info)\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')\n","print('No of CPU cores:', cpu_count())"],"id":"fzn7vlweYh25","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_wSUfJvYT79"},"source":["if COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    !pip install git+https://github.com/albumentations-team/albumentations.git"],"id":"U_wSUfJvYT79","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xGY-372X4YG"},"source":["import os\n","import cv2\n","import time\n","import json\n","import random\n","import collections\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import GroupKFold, StratifiedKFold\n","import torch\n","import torchvision\n","from torchvision.transforms import ToPILImage\n","from torchvision.transforms import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import warnings\n","if DEBUG:\n","    warnings.filterwarnings('ignore', category=UserWarning) \n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","if torch.cuda.is_available():\n","    DEVICE = torch.device('cuda')\n","    print('GPU is available')\n","else:\n","    DEVICE = torch.device('cpu')\n","    print('CPU is used')"],"id":"-xGY-372X4YG","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aqHSz9DPX4YI"},"source":["## Configs"],"id":"aqHSz9DPX4YI"},{"cell_type":"code","metadata":{"id":"jHEukSZFX4YJ"},"source":["VER = 'ver0'\n","FOLD_START = 0\n","WORK_DIR = '/content/drive/MyDrive/sartorius'\n","DATA_PATH = '../input/sartorius-cell-instance-segmentation' if KAGGLE else f'{WORK_DIR}/data'\n","MDLS_PATH = f'../input/sartorius-models-{VER}' if KAGGLE else f'{WORK_DIR}/models_{VER}'\n","CONFIG = {\n","    'width': 704,\n","    'height': 520,\n","    'resize': None,\n","    'batch_size': 2,\n","    'grad_accum': 1,\n","    'workers': 2,\n","    'folds': 5,\n","    'epochs': 4 if DEBUG else 50,\n","    'lr': 5e-4,\n","    'mask_th': .5,\n","    'normalize': False,\n","    'scheduler': True,\n","    'num_boxes': 100,\n","    'min_score': .5,\n","    'patience': 2 if DEBUG else 5,\n","    'verbose': 30,\n","    'seed': 2021\n","}\n","if not os.path.exists(MDLS_PATH):\n","    os.mkdir(MDLS_PATH)\n","with open(f'{MDLS_PATH}/config.json', 'w') as file:\n","    json.dump(CONFIG, file)\n","RESNET_MEAN = (.485, .456, .406)\n","RESNET_STD = (.229, .224, .225)\n","\n","def seed_all(seed=0):\n","    np.random.seed(seed)\n","    random_state = np.random.RandomState(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    return random_state    \n","\n","random_state = seed_all(CONFIG['seed'])\n","start_time = time.time()"],"id":"jHEukSZFX4YJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a1ngb5cGX4YO"},"source":["## Train test split"],"id":"a1ngb5cGX4YO"},{"cell_type":"code","metadata":{"id":"B8x9nqDMX4YO"},"source":["df = pd.read_csv(f'{DATA_PATH}/train.csv')\n","if DEBUG: \n","    df = df.sample(100)\n","    df.reset_index(inplace=True)\n","gkf = GroupKFold(n_splits=CONFIG['folds'])\n","df['fold'] = -1\n","for i, (train_idxs, val_idxs) in enumerate(gkf.split(df, groups=df['id'])):\n","    df.loc[val_idxs, 'fold'] = i\n","display(df.head())"],"id":"B8x9nqDMX4YO","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SRZCb1V9X4YP"},"source":["plt.figure(figsize=(16, 4))\n","plt.subplot(1, 3, 1)\n","plt.title(f'train data, {len(df.loc[df.fold != 0].id.unique())} unique imgs')\n","df.loc[df.fold != 0].cell_type.hist()\n","plt.subplot(1, 3, 2)\n","plt.title(f'val data, {len(df.loc[df.fold == 0].id.unique())} unique imgs')\n","df.loc[df.fold == 0].cell_type.hist()\n","plt.show()"],"id":"SRZCb1V9X4YP","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9M11yjwQX4YJ"},"source":["## Utilities"],"id":"9M11yjwQX4YJ"},{"cell_type":"code","metadata":{"id":"mbjhbB0tX4YK"},"source":["class Compose:\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, img, target):\n","        for t in self.transforms:\n","            img, target = t(img, target)\n","        return img, target\n","\n","class VerticalFlip:\n","    def __init__(self, prob):\n","        self.prob = prob\n","\n","    def __call__(self, img, target):\n","        if random.random() < self.prob:\n","            height, width = img.shape[-2:]\n","            img = img.flip(-2)\n","            bbox = target['boxes']\n","            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n","            target['boxes'] = bbox\n","            target['masks'] = target['masks'].flip(-2)\n","        return img, target\n","\n","class HorizontalFlip:\n","    def __init__(self, prob):\n","        self.prob = prob\n","\n","    def __call__(self, img, target):\n","        if random.random() < self.prob:\n","            height, width = img.shape[-2:]\n","            img = img.flip(-1)\n","            bbox = target['boxes']\n","            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n","            target['boxes'] = bbox\n","            target['masks'] = target['masks'].flip(-1)\n","        return img, target\n","\n","class Normalize:\n","    def __call__(self, img, target):\n","        img = F.normalize(img, RESNET_MEAN, RESNET_STD)\n","        return img, target\n","\n","class ToTensor:\n","    def __call__(self, img, target):\n","        img = F.to_tensor(img)\n","        return img, target\n","    \n","def transforms(train):\n","    transforms = [ToTensor()]\n","    if CONFIG['normalize']:\n","        transforms.append(Normalize())\n","    if train: \n","        transforms.append(HorizontalFlip(.5))\n","        transforms.append(VerticalFlip(.5))\n","    return Compose(transforms)"],"id":"mbjhbB0tX4YK","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CjdtRtIWX4YM"},"source":["def rle_decode(mask_rle, shape, color=1):\n","    \"\"\"\n","    Converts string to mask\n","    mask_rle: run-length as string formated (start length)\n","    hape: (height, width) of array to return \n","    Returns numpy array, 1 - mask, 0 - background\n","    \n","    \"\"\"\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n","    for lo, hi in zip(starts, ends):\n","        img[lo : hi] = color\n","    return img.reshape(shape)"],"id":"CjdtRtIWX4YM","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5t_m0bfpX4YM"},"source":["## Datasets and loaders"],"id":"5t_m0bfpX4YM"},{"cell_type":"code","metadata":{"id":"UGVqpNBHX4YN"},"source":["class CellInferDataset(Dataset):\n","    def __init__(self, img_dir, transforms=None):\n","        self.transforms = transforms\n","        self.img_dir = img_dir\n","        self.img_idxs = [x[:-4] for x in os.listdir(self.img_dir)]\n","    \n","    def __getitem__(self, idx):\n","        img_idx = self.img_idxs[idx]\n","        img_path = os.path.join(self.img_dir, img_idx + '.png')\n","        img = Image.open(img_path).convert('RGB')\n","        if self.transforms is not None:\n","            img, _ = self.transforms(img=img, target=None)\n","        return {'image': img, 'image_id': img_idx}\n","\n","    def __len__(self):\n","        return len(self.img_ids)\n","\n","class CellDataset(Dataset):\n","    def __init__(self, img_dir, df, transforms=None, aug=None):\n","        self.transforms = transforms\n","        self.aug = aug\n","        self.img_dir = img_dir\n","        self.df = df\n","        self.height = CONFIG['height']\n","        self.width = CONFIG['width'] \n","        self.img_info = collections.defaultdict(dict)\n","        temp_df = self.df.groupby('id')['annotation'].agg(\n","            lambda x: list(x)\n","        ).reset_index()\n","        for index, row in temp_df.iterrows():\n","            self.img_info[index] = {\n","                'image_id': row['id'],\n","                'image_path': os.path.join(self.img_dir, row['id'] + '.png'),\n","                'annotations': row[\"annotation\"]\n","            }\n","    \n","    def bbox_from_mask(self, mask):\n","        pos = np.where(mask)\n","        xmin = np.min(pos[1])\n","        xmax = np.max(pos[1])\n","        ymin = np.min(pos[0])\n","        ymax = np.max(pos[0])\n","        return [xmin, ymin, xmax, ymax]\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_info[idx][\"image_path\"]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        info = self.img_info[idx]\n","        n_objects = len(info['annotations'])\n","        boxes = []\n","        masks = []\n","        for i, ann in enumerate(info['annotations']):\n","            mask = rle_decode(ann, (self.height, self.width))\n","            mask = Image.fromarray(mask)\n","            mask = np.array(mask) > 0\n","            boxes.append(self.bbox_from_mask(mask))\n","            mask = np.array(mask).astype(np.float32)\n","            masks.append(mask)\n","        labels = [1 for _ in range(n_objects)]\n","        if self.aug:\n","            img = np.array(img).astype(np.float32) / 255\n","            augmented = self.aug(image=img, masks=masks, \n","                                 bboxes=boxes, class_labels=labels)\n","            img = augmented['image']\n","            masks = augmented['masks']\n","            boxes = augmented['bboxes']\n","            labels = augmented['class_labels']\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","        masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n","        img_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        iscrowd = torch.zeros((len(labels), ), dtype=torch.int64)\n","        target = {\n","            'boxes': boxes,\n","            'labels': labels,\n","            'masks': masks,\n","            'image_id': img_id,\n","            'area': area,\n","            'iscrowd': iscrowd\n","        }\n","        if self.transforms:\n","            img, target = self.transforms(img, target)\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.img_info)"],"id":"UGVqpNBHX4YN","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T--Ni15tX4YP"},"source":["## Model"],"id":"T--Ni15tX4YP"},{"cell_type":"code","metadata":{"id":"SxGJqVryX4YP"},"source":["def cell_model(pretrained=True, pretrained_backbone=True):\n","    N_CLASSES = 2\n","    if CONFIG['normalize']:\n","        model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n","            pretrained=pretrained, \n","            pretrained_backbone=pretrained_backbone,\n","            box_detections_per_img=CONFIG['num_boxes'],\n","            image_mean=RESNET_MEAN, \n","            image_std=RESNET_STD\n","        )\n","    else:\n","        model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n","            pretrained=pretrained,\n","            pretrained_backbone=pretrained_backbone,\n","            box_detections_per_img=CONFIG['num_boxes']\n","        )\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, N_CLASSES)\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n","        in_features_mask, \n","        hidden_layer, \n","        N_CLASSES\n","    )\n","    return model"],"id":"SxGJqVryX4YP","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K60JBbdRX4YQ"},"source":["## Training"],"id":"K60JBbdRX4YQ"},{"cell_type":"code","metadata":{"id":"SdPG_j_MX4YQ"},"source":["class CellTrainer:\n","    def __init__(self, model, device, optimizer, scheduler=None):\n","        self.model = model\n","        self.device = device\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","        self.best_val_loss = np.inf\n","        self.train_losses = []\n","        self.val_losses = []\n","        self.train_mask_losses = []\n","        self.val_mask_losses = []\n","        self.lastmodel = None\n","        \n","    def fit(self, epochs, train_loader, val_loader, save_name, max_patience):     \n","        n_patience = 0\n","        for n_epoch in range(1, epochs + 1):\n","            self.info_message('EPOCH: {}', n_epoch)\n","            train_loss, train_mask_loss, train_time = self.train_epoch(train_loader)\n","            val_loss, val_mask_loss, val_time = self.val_epoch(val_loader)\n","            self.train_losses.append(train_loss)\n","            self.train_mask_losses.append(train_mask_loss)\n","            self.val_losses.append(val_loss)\n","            self.val_mask_losses.append(val_mask_loss)\n","            self.info_message(\n","                'epoch train: {} | loss: {:.4f} | mask loss: {:.4f} | time: {:.2f} sec',\n","                n_epoch, train_loss, train_mask_loss, train_time\n","            )\n","            self.info_message(\n","                'epoch val: {} | loss: {:.4f} | mask loss: {:.4f} | time: {:.2f} sec',\n","                n_epoch, val_loss, val_mask_loss, val_time\n","            )\n","            if self.best_val_loss > val_loss: \n","                self.save_model(n_epoch, save_name)\n","                self.info_message(\n","                    'val loss improved {:.4f} -> {:.4f} | saved model to \"{}\"', \n","                    self.best_val_loss, val_loss, self.lastmodel\n","                )\n","                self.best_val_loss = val_loss\n","                n_patience = 0\n","            else:\n","                n_patience += 1\n","            if n_patience >= max_patience:\n","                self.info_message(\n","                    '\\nno improvement for last {} epochs', \n","                    n_patience\n","                )\n","                break\n","        history = {\n","            'train losses': self.train_losses, \n","            'train mask losses': self.train_mask_losses, \n","            'val losses': self.val_losses,\n","            'val mask losses': self.val_mask_losses\n","        }\n","        return history\n","            \n","    def train_epoch(self, train_loader):\n","        self.model.train()\n","        scaler = torch.cuda.amp.GradScaler()\n","        t = time.time()\n","        sum_loss = 0\n","        sum_loss_mask = 0\n","        for step, (imgs, targets) in enumerate(train_loader, 1):\n","            with torch.cuda.amp.autocast():\n","                imgs = [img.to(self.device) for img in imgs]\n","                targets = [{k: v.to(self.device) \n","                            for k, v in t.items()} for t in targets]\n","                loss_dict = self.model(imgs, targets)\n","                loss = sum(loss for loss in loss_dict.values())\n","                scaler.scale(loss).backward()\n","                if ((step + 1) % CONFIG['grad_accum'] == 0) or (step + 1 == len(train_loader)):\n","                    scaler.step(self.optimizer)\n","                    scaler.update()\n","                    self.optimizer.zero_grad()\n","                    if self.scheduler:\n","                        self.scheduler.step()\n","                loss_mask = loss_dict['loss_mask']\n","                sum_loss += loss.detach().item()\n","                sum_loss_mask += loss_mask.detach().item()\n","            if step % CONFIG['verbose'] == 0:\n","                self.info_message(\n","                    'train step {}/{} | loss: {:.4f} | mask loss: {:.4f}     ',\n","                    step, len(train_loader), sum_loss / step, sum_loss_mask / step, end='\\n'\n","                )\n","        return sum_loss/len(train_loader), sum_loss_mask/len(train_loader), int(time.time()-t)\n","    \n","    def val_epoch(self, val_loader):\n","        t = time.time()\n","        sum_loss = 0\n","        sum_loss_mask = 0\n","        for step, (imgs, targets) in enumerate(val_loader, 1):\n","            with torch.no_grad():\n","                imgs = [img.to(self.device) for img in imgs]\n","                targets = [{k: v.to(self.device) \n","                            for k, v in t.items()} for t in targets]\n","                loss_dict = self.model(imgs, targets)\n","                loss = sum(loss for loss in loss_dict.values())\n","                loss_mask = loss_dict['loss_mask']\n","                sum_loss += loss.detach().item()\n","                sum_loss_mask += loss_mask.detach().item()\n","            if step % CONFIG['verbose'] == 0:\n","                self.info_message(\n","                    'val step {}/{} | loss: {:.4f} | mask loss: {:.4f}     ',\n","                    step, len(val_loader), sum_loss / step, sum_loss_mask / step, end='\\n'\n","                )\n","        return sum_loss/len(val_loader), sum_loss_mask/len(val_loader), int(time.time()-t)\n","    \n","    def save_model(self, n_epoch, save_name, loss=None, mask_loss=None):\n","        if loss:\n","            self.lastmodel = f'{MDLS_PATH}/' + \\\n","                f'{save_name}-e{n_epoch}-loss{loss:.3f}-maskloss{mask_loss:.3f}.pth'\n","        else:\n","            self.lastmodel = f'{MDLS_PATH}/{save_name}.pth'\n","        dict_save = {\n","            'model_state_dict': self.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            'n_epoch': n_epoch,\n","        }\n","        dict_save['best_val_loss'] = self.best_val_loss\n","        torch.save(dict_save, self.lastmodel)\n","    \n","    def display_plots(self):\n","        fig, axes = plt.subplots(figsize=(16, 4), nrows=1, ncols=2)\n","        axes[0].set_title('training and validation losses')\n","        axes[0].plot(self.val_losses, label='val')\n","        axes[0].plot(self.train_losses, label='train')\n","        axes[0].set_xlabel('iterations')\n","        axes[0].set_ylabel('loss')\n","        axes[0].legend()\n","        axes[1].set_title('training and validation mask losses')\n","        axes[1].plot(self.val_mask_losses, label='val')\n","        axes[1].plot(self.train_mask_losses, label='train')\n","        axes[1].set_xlabel('iterations')\n","        axes[1].set_ylabel('loss')\n","        axes[1].legend()\n","        plt.show()\n","        plt.close()\n","    \n","    @staticmethod\n","    def info_message(message, *args, end='\\n'):\n","        print(message.format(*args), end=end)"],"id":"SdPG_j_MX4YQ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_lFNpa_X4YT"},"source":["train_aug =  A.Compose([\n","    A.OneOf([\n","        A.RandomBrightnessContrast(\n","            brightness_limit=.2, \n","            contrast_limit=.2, \n","            p=1), \n","        A.RandomGamma(p=1)\n","    ], p=.25),\n","    A.Blur(blur_limit=3, p=.25),\n","    A.GaussNoise(.002, p=.25),\n","    A.HorizontalFlip(p=.5),\n","    A.VerticalFlip(p=.5),\n","    #A.ShiftScaleRotate(p=1),\n","    #A.RGBShift(p=.2),\n","    #A.Resize(CONFIG['height'], CONFIG['width'], p=1),\n","    ToTensorV2(p=1)\n","], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n","\n","val_aug =  A.Compose([\n","    ToTensorV2(p=1)\n","], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))"],"id":"F_lFNpa_X4YT","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ntjTcqd-X4YW"},"source":["def train_cell_model(df_train, df_val, fold, device, \n","                     epochs, patience, batch_size):\n","    print('=' * 20, f'FOLD {fold}', '=' * 20)\n","    print('train:', df_train.shape, '| val:', df_val.shape)\n","    train_dataset = CellDataset(\n","        f'{DATA_PATH}/train', \n","        df_train, \n","        transforms=None, # transform(train=True)\n","        aug=train_aug\n","    )\n","    val_dataset = CellDataset(\n","        f'{DATA_PATH}/train', \n","        df_val, \n","        transforms=None, # transform(train=False)\n","        aug=val_aug\n","    )\n","    train_loader = DataLoader(\n","        train_dataset, \n","        batch_size=CONFIG['batch_size'], \n","        shuffle=True,\n","        num_workers=CONFIG['workers'], \n","        collate_fn=lambda x: tuple(zip(*x)),\n","        pin_memory=True\n","    )\n","    val_loader = DataLoader(\n","        val_dataset, \n","        batch_size=CONFIG['batch_size'], \n","        shuffle=False,\n","        num_workers=CONFIG['workers'], \n","        collate_fn=lambda x: tuple(zip(*x)),\n","        pin_memory=True\n","    )\n","    model = cell_model()\n","    model.to(device)\n","    for param in model.parameters():\n","        param.requires_grad = True\n","    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'])\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","        optimizer, \n","        CONFIG['epochs']\n","    )\n","    trainer = CellTrainer(\n","        model, \n","        device, \n","        optimizer, \n","        scheduler\n","    )\n","    history = trainer.fit(\n","        epochs, \n","        train_loader, \n","        val_loader, \n","        save_name=f'model-f{fold}', \n","        max_patience=patience\n","    )\n","    trainer.display_plots()\n","    with open(f'{MDLS_PATH}/history_f{fold}.json', 'w') as file:\n","        json.dump(history, file)\n","    return trainer.lastmodel"],"id":"ntjTcqd-X4YW","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KxeyM55EX4YW"},"source":["modelfiles = []\n","for fold_num in range(FOLD_START, CONFIG['folds']): \n","    train_idxs = np.where((df['fold'] != fold_num))[0]\n","    val_idxs = np.where((df['fold'] == fold_num))[0]\n","    df_train = df.loc[train_idxs]\n","    df_val = df.loc[val_idxs]\n","    modelfiles.append(train_cell_model(\n","        df_train, \n","        df_val, \n","        fold_num,\n","        device=DEVICE, \n","        epochs=CONFIG['epochs'],\n","        patience=CONFIG['patience'],\n","        batch_size=CONFIG['batch_size']\n","    ))\n","print(modelfiles)\n","with open(f'{MDLS_PATH}/modelfiles.json', 'w') as file:\n","    json.dump(modelfiles, file)"],"id":"KxeyM55EX4YW","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eOsH4UtEX4YX"},"source":["modelfiles = [f'{MDLS_PATH}/{x.split(\"/\")[-1]}' for x in modelfiles]\n","allmodelfiles = [f'{MDLS_PATH}/{x}' for x in os.listdir(MDLS_PATH) if '.pth' in x]\n","for file_path in allmodelfiles:\n","    if file_path not in modelfiles:\n","        os.remove(file_path)"],"id":"eOsH4UtEX4YX","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4AC64yzyX4YX"},"source":["## Results"],"id":"4AC64yzyX4YX"},{"cell_type":"code","metadata":{"id":"WLiQXtBlX4YX"},"source":["def plot_img_mask_sample(model, train_dataset, idx):\n","    \"\"\"\n","    Plots: the image, image and ground truth mask, \n","    image and predicted mask\n","    \n","    \"\"\"\n","    img, targets = train_dataset[idx]\n","    print()\n","    plt.figure(figsize=(12, 4))\n","    \n","    plt.subplot(1, 3, 1)\n","    plt.imshow(img.numpy().transpose((1, 2, 0)))\n","    plt.title('image')\n","    plt.axis('off')\n","    \n","    plt.subplot(1, 3, 2)\n","    masks = np.zeros((CONFIG['height'], CONFIG['width']))\n","    for mask in targets['masks']:\n","        masks = np.logical_or(masks, mask)\n","    plt.imshow(img.numpy().transpose((1, 2, 0)))\n","    plt.imshow(masks, alpha=.3)\n","    plt.title('ground truth mask')\n","    plt.axis('off')\n","    \n","    plt.subplot(1, 3, 3)\n","    with torch.no_grad():\n","        preds = model([img.to(DEVICE)])[0]\n","    plt.imshow(img.cpu().numpy().transpose((1, 2, 0)))\n","    all_preds_masks = np.zeros((CONFIG['height'], CONFIG['width']))\n","    for mask in preds['masks'].cpu().detach().numpy():\n","        all_preds_masks = np.logical_or(all_preds_masks, mask[0] > .5)\n","    plt.imshow(all_preds_masks, alpha=.4)\n","    plt.title('predicted masks')\n","    plt.axis('off')\n","    \n","    plt.show()"],"id":"WLiQXtBlX4YX","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2SkjOy68X4YY"},"source":["train_dataset = CellDataset(\n","    f'{DATA_PATH}/train', \n","    df, \n","    transforms=None, #transforms(train=True),\n","    aug=val_aug\n",")\n","for model_file in modelfiles:\n","    model = cell_model()\n","    model.to(DEVICE)\n","    checkpoint = torch.load(model_file)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","    print(model_file)\n","    plot_img_mask_sample(model, train_dataset, idx=0)"],"id":"2SkjOy68X4YY","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8eVCfGmKX4YY"},"source":[""],"id":"8eVCfGmKX4YY","execution_count":null,"outputs":[]}]}