{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"detectron_train.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"papermill":{"default_parameters":{},"duration":2131.85871,"end_time":"2021-10-27T20:12:23.743663","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-10-27T19:36:51.884953","version":"2.3.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fpP-KGOCfFSi"},"source":["# Detectron: preprocess data COCO format"],"id":"fpP-KGOCfFSi"},{"cell_type":"code","metadata":{"id":"yV-p90BYkLPH"},"source":["!nvidia-smi"],"id":"yV-p90BYkLPH","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dvd_ww4KfFSo"},"source":["DEBUG = False\n","KAGGLE = False\n","COLAB = True"],"id":"dvd_ww4KfFSo","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ctHaxXeefFSq"},"source":["## Install"],"id":"ctHaxXeefFSq"},{"cell_type":"code","metadata":{"id":"mE0IXoOvfFSq"},"source":["import torch\n","TORCH_VER = '.'.join(torch.__version__.split('.')[:2])\n","CUDA_VER = torch.__version__.split('+')[-1]\n","print('torch:', TORCH_VER, '| cuda:', CUDA_VER)\n","if COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/{CUDA_VER}/torch{TORCH_VER}/index.html\n","    !pip install wandb\n","else:\n","    print(\n","        'install with command:\\n'\n","        'pip install detectron2 -f'\n","        f'https://dl.fbaipublicfiles.com/detectron2/wheels/{CUDA_VER}/torch{TORCH_VER}/index.html'\n","    )"],"id":"mE0IXoOvfFSq","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_7NMzi0fFSr"},"source":["import os\n","import cv2\n","import json\n","import time\n","import random\n","import wandb\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import pycocotools.mask as mask_util\n","import detectron2\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor, DefaultTrainer, hooks, BestCheckpointer\n","from detectron2.config import get_cfg, CfgNode\n","from detectron2.utils.visualizer import Visualizer, ColorMode\n","from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n","from detectron2.data.datasets import register_coco_instances\n","from detectron2.utils.logger import setup_logger\n","from detectron2.evaluation.evaluator import DatasetEvaluator\n","import detectron2.data.transforms as T\n","from detectron2.data import DatasetMapper, build_detection_train_loader\n","from detectron2.evaluation import inference_on_dataset\n","from detectron2.checkpoint import DetectionCheckpointer\n","import warnings\n","if DEBUG:\n","    warnings.filterwarnings('ignore', category=UserWarning) \n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","if torch.cuda.is_available():\n","    DEVICE = torch.device('cuda')\n","    print('GPU is available')\n","else:\n","    DEVICE = torch.device('cpu')\n","    print('CPU is used')\n","setup_logger()"],"id":"T_7NMzi0fFSr","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"85-Fw3HofFSt"},"source":["## Config"],"id":"85-Fw3HofFSt"},{"cell_type":"code","metadata":{"id":"LbKPBT_YfFSt"},"source":["VER = 'ver0'\n","WORK_DIR = '/content/drive/MyDrive/sartorius' if COLAB else '.'\n","DATA_PATH = '../input/sartorius-cell-instance-segmentation' if KAGGLE else f'{WORK_DIR}/data'\n","MDLS_PATH = f'../input/sartorius-models-{VER}' if KAGGLE else f'{WORK_DIR}/models_{VER}'\n","CONFIG = {\n","    'ver': VER,\n","    'fold': 4,\n","    'folds': 5,\n","    'batch_size': 4,\n","    'workers': 4 if COLAB else 8,\n","    #'chk_point': 'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml',\n","    'chk_point': 'COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml',\n","    #'chk_point': 'COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml',\n","    #'start_weights': None,\n","    #'start_weights': '/content/drive/MyDrive/sartorius/models_vclbdtrn7lc/model_best.pth',\n","    'start_weights': '/content/drive/MyDrive/sartorius/models_vclbdtrn7lc/model_0006503.pth',\n","    'chk_point_period': False,\n","    'eval': True,\n","    'lr': 1e-3, # 1e-3 = default\n","    'epochs': 4 if DEBUG else 40, # 40000 = default\n","    'warmup_factor': 1e-6,\n","    'warm_up_ep': 5,\n","    'gamma': .9,\n","    'gamma_step': .1,\n","    'score_th': .5,\n","    'batch_size_per_img': 128, # number of regions per image used to train RPN\n","    'lc': False,\n","    'ext': False,\n","    'clr_aug': True,\n","    'classes': 3,\n","    'freeze_at': None,\n","    'ups': 'ups1',\n","    'seed': 2021\n","}\n","if not os.path.exists(MDLS_PATH):\n","    os.mkdir(MDLS_PATH)\n","with open(f'{MDLS_PATH}/config.json', 'w') as file:\n","    json.dump(CONFIG, file)\n","\n","def seed_all(seed=0):\n","    np.random.seed(seed)\n","    random_state = np.random.RandomState(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    return random_state    \n","\n","with open(f'{WORK_DIR}/../.wandb', 'r') as file:\n","    api_key = file.read()\n","wandb.login(key=api_key)\n","wandb.init(\n","    project='sartorius',\n","    sync_tensorboard=True, \n","    name=f'Sartorius detectron2 {VER}',\n","    settings=wandb.Settings(\n","        start_method=\"thread\", \n","        console=\"auto\"\n","    )\n",")\n","wandb.config = CONFIG  \n","    \n","random_state = seed_all(CONFIG['seed'])\n","start_time = time.time()"],"id":"LbKPBT_YfFSt","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lIxTgICdfFSv"},"source":["## Data description"],"id":"lIxTgICdfFSv"},{"cell_type":"code","metadata":{"id":"4tABPfNGfFSw"},"source":["df = pd.read_csv(f'{DATA_PATH}/train.csv')"],"id":"4tABPfNGfFSw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4do-6jN8fFSw"},"source":["df_instances = df.groupby(['id']).agg({\n","    'annotation': 'count', \n","    'cell_type': 'first'\n","})\n","df_instances = df_instances.groupby(\"cell_type\")[['annotation']].describe(\n","    percentiles=[.1, .25, .75, .8, .85, .9, .95, .99]\n",").astype(int).T.droplevel(level=0).T.drop([\n","    'count', \n","    '50%', \n","    'std'\n","], axis=1)\n","display(df_instances)"],"id":"4do-6jN8fFSw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"68uJh_JZfFSx"},"source":["df['n_pixels'] = df.annotation.apply(\n","    lambda x: np.sum([int(e) for e in x.split()[1:][::2]])\n",")\n","df_pixels = df.groupby(\"cell_type\")[['n_pixels']].describe(\n","    percentiles=[.01, .02, .05, .1, .9, .95, .98, .99]\n",").astype(int).T.droplevel(level=0).T.drop([\n","    'count', \n","    '50%', \n","    'std'\n","], axis=1)\n","display(df_pixels)"],"id":"68uJh_JZfFSx","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LBgdkgw0fFSy"},"source":["## Load data"],"id":"LBgdkgw0fFSy"},{"cell_type":"code","metadata":{"id":"vK2sOLThfFSy"},"source":["cfg = get_cfg()\n","if CONFIG['lc']:\n","    register_coco_instances(\n","        'sartorius_train',\n","        {}, \n","        f'{DATA_PATH}/data2/livecell_annotations_train.json', \n","        f'{DATA_PATH}/data2/LIVECell_dataset_2021/images'\n","    )\n","    register_coco_instances(\n","        'sartorius_val',\n","        {},\n","        f'{DATA_PATH}/data2/livecell_annotations_val.json', \n","        f'{DATA_PATH}/data2/LIVECell_dataset_2021/images'\n","    )\n","else:\n","    cfg.INPUT.MASK_FORMAT = 'bitmask'\n","    if CONFIG['ext']:\n","        file_tr = f'{DATA_PATH}/coco_annotations_train_f{CONFIG[\"folds\"]}_f{CONFIG[\"fold\"]}_{CONFIG[\"ext\"]}.json'\n","        file_vl = f'{DATA_PATH}/coco_annotations_val_f{CONFIG[\"folds\"]}_f{CONFIG[\"fold\"]}_{CONFIG[\"ext\"]}.json'\n","    else:\n","        if CONFIG['ups']:\n","            file_tr = f'{DATA_PATH}/coco_annotations_train_f{CONFIG[\"folds\"]}_f{CONFIG[\"fold\"]}_{CONFIG[\"ups\"]}.json'\n","        else:\n","            file_tr = f'{DATA_PATH}/coco_annotations_train_f{CONFIG[\"folds\"]}_f{CONFIG[\"fold\"]}.json'\n","        file_vl = f'{DATA_PATH}/coco_annotations_val_f{CONFIG[\"folds\"]}_f{CONFIG[\"fold\"]}.json'\n","    register_coco_instances(\n","        'sartorius_train',\n","        {}, \n","        file_tr, \n","        DATA_PATH\n","    )\n","    register_coco_instances(\n","        'sartorius_val',\n","        {},\n","        file_vl, \n","        DATA_PATH\n","    )\n","metadata = MetadataCatalog.get('sartorius_train')\n","ds_train = DatasetCatalog.get('sartorius_train')"],"id":"vK2sOLThfFSy","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"scruv63VfFSy"},"source":["## Demo sample data"],"id":"scruv63VfFSy"},{"cell_type":"code","metadata":{"id":"fEnyPSKlfFSz"},"source":["demo_dict = ds_train[0]\n","img = cv2.imread(demo_dict['file_name'])\n","visualizer = Visualizer(img[:, :, ::-1], metadata=metadata)\n","out = visualizer.draw_dataset_dict(demo_dict)\n","plt.figure(figsize=(12, 8))\n","plt.imshow(out.get_image()[:, :, ::-1])\n","plt.axis('off')\n","plt.show()"],"id":"fEnyPSKlfFSz","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NX3uux8UfFSz"},"source":["## Evaluator "],"id":"NX3uux8UfFSz"},{"cell_type":"code","metadata":{"id":"hvC66ZgzfFS0"},"source":["from detectron2.structures import polygons_to_bitmask\n","\n","def polygon_to_rle(polygon, shape=(520, 704)):\n","    mask = polygons_to_bitmask([np.asarray(polygon) + 0.25], shape[0], shape[1])\n","    rle = mask_util.encode(np.asfortranarray(mask))\n","    return rle\n","\n","def precision_at(threshold, iou):\n","    matches = iou > threshold\n","    true_pos = np.sum(matches, axis=1) == 1  # Correct objects\n","    false_pos = np.sum(matches, axis=0) == 0  # Missed objects\n","    false_neg = np.sum(matches, axis=1) == 0  # Extra objects\n","    return np.sum(true_pos), np.sum(false_pos), np.sum(false_neg)\n","\n","def score(pred, targ):\n","    pred_masks = pred['instances'].pred_masks.cpu().numpy()\n","    enc_preds = [\n","        mask_util.encode(np.asarray(p, order='F')) \n","        for p in pred_masks\n","    ]\n","    enc_targs = list(map(lambda x: x['segmentation'], targ))\n","    if CONFIG['lc']:\n","        enc_targs = [polygon_to_rle(enc_targ[0]) for enc_targ in enc_targs]\n","    ious = mask_util.iou(enc_preds, enc_targs, [0] * len(enc_targs))\n","    prec = []\n","    for t in np.arange(.5, 1, .05):\n","        tp, fp, fn = precision_at(t, ious)\n","        p = tp / (tp + fp + fn)\n","        prec.append(p)\n","    return np.mean(prec)\n","\n","class MAPIOUEvaluator(DatasetEvaluator):\n","    def __init__(self, dataset_name):\n","        dataset_dicts = DatasetCatalog.get(dataset_name)\n","        self.annotations_cache = {\n","            item['image_id']: item['annotations'] \n","            for item in dataset_dicts\n","        }\n","    \n","    def reset(self):\n","        self.scores = []\n","\n","    def process(self, inputs, outputs):\n","        for inp, out in zip(inputs, outputs):\n","            if len(out['instances']) == 0:\n","                self.scores.append(0)    \n","            else:\n","                targ = self.annotations_cache[inp['image_id']]\n","                self.scores.append(score(out, targ))\n","\n","    def evaluate(self):\n","        return {'MaP IoU': np.mean(self.scores)}\n","\n","class MAPIOUCEvaluator(DatasetEvaluator):\n","    def __init__(self, dataset_name, classes=CONFIG['classes']):\n","        dataset_dicts = DatasetCatalog.get(dataset_name)\n","        self.annotations_cache = {\n","            item['image_id']: item['annotations'] \n","            for item in dataset_dicts\n","        }\n","        self.classes = classes\n","    \n","    def reset(self):\n","        self.scores = []\n","\n","    def process(self, inputs, outputs):\n","        for inp, out in zip(inputs, outputs):\n","            if len(out['instances']) == 0:\n","                self.scores.append([0] * self.classes)    \n","            else:\n","                targ = self.annotations_cache[inp['image_id']]\n","                class_scores = []\n","                for c in range(self.classes):\n","                    targ_c = [x  for x in targ if x['category_id'] == c]\n","                    if targ_c:\n","                        class_scores.append(score(out, targ_c))\n","                    else:\n","                        class_scores.append(0)\n","                self.scores.append(class_scores)\n","\n","    def evaluate(self):\n","        return {'MaP IoU': np.mean(self.scores, axis=0).tolist()}\n","\n","class Trainer(DefaultTrainer):\n","    @classmethod\n","    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n","        return MAPIOUEvaluator(dataset_name)\n","    \n","    @classmethod\n","    def build_train_loader(cls, cfg):\n","        '''\n","        Base aug:\n","        [DatasetMapper] Augmentations used in training: \n","        [ResizeShortestEdge(\n","            short_edge_length=(640, 672, 704, 736, 768, 800), \n","            max_size=1333, \n","            sample_style='choice'), \n","        RandomFlip()]\n","    \n","        '''\n","        augs = [\n","            T.ResizeShortestEdge(\n","                #short_edge_length=(640, 672, 704, 736, 768, 800), \n","                short_edge_length=(520, 640, 672, 704, 736, 768, 800), \n","                max_size=1333,\n","                #max_size=704, \n","                sample_style='choice'\n","                ),\n","            T.RandomFlip(prob=.5, horizontal=True, vertical=False),\n","            T.RandomFlip(prob=.5, horizontal=False, vertical=True),\n","            T.RandomApply(\n","                T.RandomCrop(crop_type='relative_range', crop_size=(.75, .75)),\n","                prob=.25\n","            )\n","        ]\n","        if CONFIG['clr_aug']:\n","            augs.extend([\n","                T.RandomApply(\n","                    T.RandomBrightness(.9, 1.1),\n","                    prob=.25\n","                ),\n","                T.RandomApply(\n","                    T.RandomContrast(.9, 1.1),\n","                    prob=.25\n","                )\n","            ])\n","        mapper = DatasetMapper(\n","            cfg, \n","            is_train=True, \n","            augmentations=augs\n","        )\n","        return build_detection_train_loader(cfg, mapper=mapper)\n","    \n","    def build_hooks(self):\n","        cfg = self.cfg.clone()\n","        hooks = super().build_hooks()\n","        hooks.insert(\n","            -1, \n","            BestCheckpointer(\n","                cfg.TEST.EVAL_PERIOD, \n","                DetectionCheckpointer(self.model, cfg.OUTPUT_DIR),\n","                'MaP IoU',\n","                'max'\n","            )\n","        )\n","        return hooks"],"id":"hvC66ZgzfFS0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJpr-dHUfFS1"},"source":["## Train"],"id":"EJpr-dHUfFS1"},{"cell_type":"code","metadata":{"id":"nHL7zBWafFS1"},"source":["iters_per_epoch = len(DatasetCatalog.get('sartorius_train')) // CONFIG['batch_size']\n","\n","cfg.merge_from_file(model_zoo.get_config_file(CONFIG['chk_point']))\n","cfg.DATASETS.TRAIN = ('sartorius_train', )\n","cfg.DATASETS.TEST = ('sartorius_val', )\n","cfg.DATALOADER.NUM_WORKERS = CONFIG['workers']\n","if CONFIG['start_weights']:\n","    cfg.MODEL.WEIGHTS = CONFIG['start_weights']\n","else:\n","    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(CONFIG['chk_point'])\n","\n","cfg.SOLVER.IMS_PER_BATCH = CONFIG['batch_size']\n","cfg.SOLVER.MAX_ITER = CONFIG['epochs'] * iters_per_epoch\n","cfg.SOLVER.CHECKPOINT_PERIOD = iters_per_epoch if CONFIG['chk_point_period'] else (cfg.SOLVER.MAX_ITER + 1)\n","cfg.SOLVER.BASE_LR = CONFIG['lr']\n","cfg.SOLVER.WARMUP_FACTOR = CONFIG['warmup_factor']\n","cfg.SOLVER.WARMUP_ITERS = iters_per_epoch * CONFIG['warm_up_ep']\n","cfg.SOLVER.WARMUP_METHOD = 'linear'\n","cfg.SOLVER.GAMMA = CONFIG['gamma']\n","steps = []\n","for i in np.arange(CONFIG['gamma_step'], 1, CONFIG['gamma_step']):\n","    steps.append(iters_per_epoch * CONFIG['warm_up_ep'] + int(cfg.SOLVER.MAX_ITER * i))\n","print('gamma steps:', steps)\n","cfg.SOLVER.STEPS = steps\n","cfg.SOLVER.AMP.ENABLED = True\n","\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = CONFIG['batch_size_per_img']   \n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = CONFIG['classes']\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = CONFIG['score_th']\n","if CONFIG['freeze_at']:\n","    cfg.MODEL.BACKBONE.FREEZE_AT = CONFIG['freeze_at']\n","\n","if CONFIG['eval']:\n","    cfg.TEST.EVAL_PERIOD = iters_per_epoch\n","\n","cfg.OUTPUT_DIR = MDLS_PATH"],"id":"nHL7zBWafFS1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"89Ang1VNfFS2"},"source":["print(\n","    'train:', len(DatasetCatalog.get('sartorius_train')),\n","    '| val:', len(DatasetCatalog.get('sartorius_val'))\n",")\n","trainer = Trainer(cfg) \n","trainer.resume_or_load(resume=False)\n","trainer.train()\n","\n","elapsed_time = time.time() - start_time\n","print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"],"id":"89Ang1VNfFS2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gi_r8DOVfFS2"},"source":["## Best model"],"id":"Gi_r8DOVfFS2"},{"cell_type":"code","metadata":{"id":"Tdluf40vfFS3"},"source":["with open(f'{MDLS_PATH}/metrics.json', 'r') as file:\n","    lines = file.readlines()\n","    metrics = [json.loads(line.rstrip())\n","               for line in lines if 'MaP IoU' in line]\n","df = pd.DataFrame(metrics)"],"id":"Tdluf40vfFS3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uh1RZkMWfFS3"},"source":["plt.figure(figsize=(16, 4))\n","plt.plot(df['MaP IoU'])\n","plt.xlabel('iters')\n","plt.xticks(\n","    list(range(len(df)))[::2], \n","    df['iteration'][::2],\n","    rotation=90\n",")\n","plt.title(f'MaP IoU, max={np.max(df[\"MaP IoU\"]):.4f}')\n","plt.show()"],"id":"uh1RZkMWfFS3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Raa3E2CRfFS3"},"source":["if CONFIG['chk_point_period']:\n","    model_files = sorted([x for x in os.listdir(f'{MDLS_PATH}') if '.pth' in x])\n","    best_model_file = model_files[np.argmax(df['MaP IoU'])]\n","    print('best model file:', best_model_file)\n","\n","    for file_name in model_files:\n","        if file_name != best_model_file:\n","            os.remove(f'{MDLS_PATH}/{file_name}')\n","else:\n","    best_model_file = 'model_best.pth'\n","best_iter = df.loc[np.argmax(df['MaP IoU']), 'iteration']"],"id":"Raa3E2CRfFS3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8BHo-eiFfFS4"},"source":["with open(f'{MDLS_PATH}/best_model.json', 'w') as file:\n","    json.dump({\n","        'file': best_model_file,\n","        'score': np.max(df[\"MaP IoU\"]),\n","        'best_iter': int(best_iter)\n","    }, file)"],"id":"8BHo-eiFfFS4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tYq4YI2RfFS4"},"source":["## Best thresholds"],"id":"tYq4YI2RfFS4"},{"cell_type":"code","metadata":{"id":"vfQmbm9DfFS4"},"source":["dataset_dicts = DatasetCatalog.get('sartorius_val')\n","val_loader = build_detection_test_loader(cfg, 'sartorius_val')"],"id":"vfQmbm9DfFS4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KdnjnWG8fFS5"},"source":["ths_scores = []\n","best_model_file = 'model_best.pth'\n","for th in np.arange(.05, 1, .05):\n","    cfg.merge_from_file(model_zoo.get_config_file(CONFIG['chk_point']))\n","    cfg.INPUT.MASK_FORMAT = 'bitmask'\n","    cfg.MODEL.ROI_HEADS.NUM_CLASSES = CONFIG['classes']\n","    cfg.MODEL.WEIGHTS = f'{MDLS_PATH}/{best_model_file}'  \n","    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = float(th)\n","    cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n","    model = DefaultPredictor(cfg)\n","    infer_result = inference_on_dataset(\n","        model.model, \n","        val_loader, \n","        MAPIOUCEvaluator('sartorius_val')\n","    )\n","    print(th, '->', infer_result)\n","    th_scores = [th]\n","    th_scores.extend(infer_result['MaP IoU'])\n","    ths_scores.append(th_scores)"],"id":"KdnjnWG8fFS5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dcDvBMS2fFS5"},"source":["ths = []\n","all_ths = [f'{x[0]:.2f}' for x in ths_scores]\n","plt.figure(figsize=(16, 4))\n","for cls in range(CONFIG['classes']):\n","    cls_scores = [x[1 + cls] for x in ths_scores]\n","    plt.plot(cls_scores)\n","    ths.append(float(all_ths[np.argmax(cls_scores)]))\n","plt.xlabel('ths')\n","plt.xticks(\n","    list(range(len(ths_scores)))[::2], \n","    all_ths[::2],\n","    rotation=90\n",")\n","plt.title(f'MaP IoU, best ths are {ths}')\n","plt.show()"],"id":"dcDvBMS2fFS5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTtG3g2yfFS6"},"source":["with open(f'{MDLS_PATH}/ths.json', 'w') as file:\n","    json.dump(ths, file)"],"id":"VTtG3g2yfFS6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tv_ppbBlfFS6"},"source":["## Inference demo"],"id":"Tv_ppbBlfFS6"},{"cell_type":"code","metadata":{"id":"A6d1MPJrfFS6"},"source":["cfg.MODEL.WEIGHTS = f'{MDLS_PATH}/{best_model_file}'\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .5\n","predictor = DefaultPredictor(cfg)\n","preds = []\n","gts = []\n","for ds_dict in random.sample(dataset_dicts, 3):    \n","    img = cv2.imread(ds_dict['file_name'])\n","    outputs = predictor(img)\n","    v = Visualizer(\n","        img[:, :, ::-1],\n","        metadata = MetadataCatalog.get('sartorius_train'), \n","        instance_mode=ColorMode.IMAGE_BW\n","    )\n","    pred = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","    visualizer = Visualizer(\n","        img[:, :, ::-1], \n","        metadata=MetadataCatalog.get('sartorius_train')\n","    )\n","    target = visualizer.draw_dataset_dict(ds_dict)\n","    preds.append(pred)\n","    gts.append(target)\n","\n","fig, axs = plt.subplots(len(preds), 2, figsize=(16, 6 * len(preds)))\n","for i, ax in enumerate(axs):\n","    ax[0].imshow(gts[i].get_image()[:, :, ::-1])\n","    ax[0].set_title('ground truth')\n","    ax[0].axis('off')\n","    ax[1].imshow(preds[i].get_image()[:, :, ::-1])\n","    ax[1].set_title('preds')\n","    ax[1].axis('off')"],"id":"A6d1MPJrfFS6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QuNb97xjfFS6"},"source":[""],"id":"QuNb97xjfFS6","execution_count":null,"outputs":[]}]}